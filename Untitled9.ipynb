{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-fYY8CvSs08",
        "outputId": "a7f8c010-3fd6-492f-f7ee-eed045c6e7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 256, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "mean = torch.load(\"/content/drive/MyDrive/OASIS2/OAS2_RAW_PART1/mean_t.pt\")\n",
        "std = torch.load( \"/content/drive/MyDrive/OASIS2/OAS2_RAW_PART1/std_t.pt\")\n",
        "mean.reshape(256,256,128)\n",
        "mean.size()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "my_dataframe = pd.read_csv(\"/content/drive/MyDrive/OASIS2/OAS2_RAW_PART1/Oasis_Demo_2 - Oasis_Demo.csv\")\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "\n",
        "class createDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transforms.Compose([transforms.Normalize(torch.reshape(mean,(256,256,128,1)), torch.reshape(std,(256,256,128,1))), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.dataframe.iloc[index, 0]\n",
        "        image = nib.load(image).get_fdata()  \n",
        "        image = torch.from_numpy(np.asarray(image))\n",
        "        image = image.double()\n",
        "        image = self.transform(image)\n",
        "        #image = image.squeeze()\n",
        "        image = image.double()\n",
        "\n",
        "        label = self.dataframe.iloc[index, 1]\n",
        "        label = torch.from_numpy(np.array(label, dtype='int32'))\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataframe.shape[0]\n",
        "        \n",
        "      \n",
        "my_dataset = createDataset(dataframe = my_dataframe)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "my_dataloader = DataLoader(dataset = my_dataset, batch_size=3, shuffle = True, )\n",
        "train_dataloader = DataLoader(dataset = my_dataset, batch_size=3, shuffle = True, length = 0.8 )"
      ],
      "metadata": {
        "id": "ZzIr7fgBSxsx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample):\n",
        "        super().__init__()\n",
        "        if downsample:\n",
        "            self.conv1 = nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                               kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, input):\n",
        "        shortcut = self.shortcut(input)\n",
        "        input = nn.ReLU()(self.bn1(self.conv1(input)))\n",
        "        input = nn.ReLU()(self.bn2(self.conv2(input)))\n",
        "        input = input + shortcut\n",
        "        return nn.ReLU()(input)\n",
        "class ResBottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels//4,\n",
        "                               kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels//4, out_channels//4, kernel_size=3, stride=2 if downsample else 1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(out_channels//4, out_channels, kernel_size=1, stride=1)\n",
        "\n",
        "        if self.downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=2 if self.downsample else 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels//4)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels//4)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, input):\n",
        "        shortcut = self.shortcut(input)\n",
        "        input = nn.ReLU()(self.bn1(self.conv1(input)))\n",
        "        input = nn.ReLU()(self.bn2(self.conv2(input)))\n",
        "        input = nn.ReLU()(self.bn3(self.conv3(input)))\n",
        "        input = input + shortcut\n",
        "        return nn.ReLU()(input)\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, resblock, repeat, useBottleneck=False, outputs=1000):\n",
        "        super().__init__()\n",
        "        self.layer0 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        if useBottleneck:\n",
        "            filters = [64, 256, 512, 1024, 2048]\n",
        "        else:\n",
        "            filters = [64, 64, 128, 256, 512]\n",
        "\n",
        "        self.layer1 = nn.Sequential()\n",
        "        self.layer1.add_module('conv2_1', resblock(filters[0], filters[1], downsample=False))\n",
        "        for i in range(1, repeat[0]):\n",
        "                self.layer1.add_module('conv2_%d'%(i+1,), resblock(filters[1], filters[1], downsample=False))\n",
        "\n",
        "        self.layer2 = nn.Sequential()\n",
        "        self.layer2.add_module('conv3_1', resblock(filters[1], filters[2], downsample=True))\n",
        "        for i in range(1, repeat[1]):\n",
        "                self.layer2.add_module('conv3_%d' % (\n",
        "                    i+1,), resblock(filters[2], filters[2], downsample=False))\n",
        "\n",
        "        self.layer3 = nn.Sequential()\n",
        "        self.layer3.add_module('conv4_1', resblock(filters[2], filters[3], downsample=True))\n",
        "        for i in range(1, repeat[2]):\n",
        "            self.layer3.add_module('conv2_%d' % (\n",
        "                i+1,), resblock(filters[3], filters[3], downsample=False))\n",
        "\n",
        "        self.layer4 = nn.Sequential()\n",
        "        self.layer4.add_module('conv5_1', resblock(filters[3], filters[4], downsample=True))\n",
        "        for i in range(1, repeat[3]):\n",
        "            self.layer4.add_module('conv3_%d'%(i+1,),resblock(filters[4], filters[4], downsample=False))\n",
        "\n",
        "        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = torch.nn.Linear(filters[4], outputs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = self.layer0(input)\n",
        "        input = self.layer1(input)\n",
        "        input = self.layer2(input)\n",
        "        input = self.layer3(input)\n",
        "        input = self.layer4(input)\n",
        "        input = self.gap(input)\n",
        "        # torch.flatten()\n",
        "        # https://stackoverflow.com/questions/60115633/pytorch-flatten-doesnt-maintain-batch-size\n",
        "        input = torch.flatten(input, start_dim=1)\n",
        "        input = self.fc(input)\n",
        "\n",
        "        return input\n",
        "from torchsummary import summary\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "resnet152 = ResNet(256, ResBottleneckBlock, [\n",
        "                   3, 8, 36, 3], useBottleneck=True, outputs=1000)\n",
        "resnet152.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
        "summary(resnet152, (256, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umhexFnyTuX4",
        "outputId": "b79d46ad-6d7e-4c82-c6b8-9674e0955e2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]         802,880\n",
            "         MaxPool2d-2           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-3           [-1, 64, 56, 56]             128\n",
            "              ReLU-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5          [-1, 256, 56, 56]          16,640\n",
            "       BatchNorm2d-6          [-1, 256, 56, 56]             512\n",
            "            Conv2d-7           [-1, 64, 56, 56]           4,160\n",
            "       BatchNorm2d-8           [-1, 64, 56, 56]             128\n",
            "            Conv2d-9           [-1, 64, 56, 56]          36,928\n",
            "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,640\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "ResBottleneckBlock-13          [-1, 256, 56, 56]               0\n",
            "           Conv2d-14           [-1, 64, 56, 56]          16,448\n",
            "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
            "           Conv2d-16           [-1, 64, 56, 56]          36,928\n",
            "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
            "           Conv2d-18          [-1, 256, 56, 56]          16,640\n",
            "      BatchNorm2d-19          [-1, 256, 56, 56]             512\n",
            "ResBottleneckBlock-20          [-1, 256, 56, 56]               0\n",
            "           Conv2d-21           [-1, 64, 56, 56]          16,448\n",
            "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
            "           Conv2d-23           [-1, 64, 56, 56]          36,928\n",
            "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
            "           Conv2d-25          [-1, 256, 56, 56]          16,640\n",
            "      BatchNorm2d-26          [-1, 256, 56, 56]             512\n",
            "ResBottleneckBlock-27          [-1, 256, 56, 56]               0\n",
            "           Conv2d-28          [-1, 512, 28, 28]         131,584\n",
            "      BatchNorm2d-29          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-30          [-1, 128, 56, 56]          32,896\n",
            "      BatchNorm2d-31          [-1, 128, 56, 56]             256\n",
            "           Conv2d-32          [-1, 128, 28, 28]         147,584\n",
            "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
            "           Conv2d-34          [-1, 512, 28, 28]          66,048\n",
            "      BatchNorm2d-35          [-1, 512, 28, 28]           1,024\n",
            "ResBottleneckBlock-36          [-1, 512, 28, 28]               0\n",
            "           Conv2d-37          [-1, 128, 28, 28]          65,664\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "           Conv2d-39          [-1, 128, 28, 28]         147,584\n",
            "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
            "           Conv2d-41          [-1, 512, 28, 28]          66,048\n",
            "      BatchNorm2d-42          [-1, 512, 28, 28]           1,024\n",
            "ResBottleneckBlock-43          [-1, 512, 28, 28]               0\n",
            "           Conv2d-44          [-1, 128, 28, 28]          65,664\n",
            "      BatchNorm2d-45          [-1, 128, 28, 28]             256\n",
            "           Conv2d-46          [-1, 128, 28, 28]         147,584\n",
            "      BatchNorm2d-47          [-1, 128, 28, 28]             256\n",
            "           Conv2d-48          [-1, 512, 28, 28]          66,048\n",
            "      BatchNorm2d-49          [-1, 512, 28, 28]           1,024\n",
            "ResBottleneckBlock-50          [-1, 512, 28, 28]               0\n",
            "           Conv2d-51          [-1, 128, 28, 28]          65,664\n",
            "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
            "           Conv2d-53          [-1, 128, 28, 28]         147,584\n",
            "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
            "           Conv2d-55          [-1, 512, 28, 28]          66,048\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "ResBottleneckBlock-57          [-1, 512, 28, 28]               0\n",
            "           Conv2d-58          [-1, 128, 28, 28]          65,664\n",
            "      BatchNorm2d-59          [-1, 128, 28, 28]             256\n",
            "           Conv2d-60          [-1, 128, 28, 28]         147,584\n",
            "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
            "           Conv2d-62          [-1, 512, 28, 28]          66,048\n",
            "      BatchNorm2d-63          [-1, 512, 28, 28]           1,024\n",
            "ResBottleneckBlock-64          [-1, 512, 28, 28]               0\n",
            "           Conv2d-65          [-1, 128, 28, 28]          65,664\n",
            "      BatchNorm2d-66          [-1, 128, 28, 28]             256\n",
            "           Conv2d-67          [-1, 128, 28, 28]         147,584\n",
            "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
            "           Conv2d-69          [-1, 512, 28, 28]          66,048\n",
            "      BatchNorm2d-70          [-1, 512, 28, 28]           1,024\n",
            "ResBottleneckBlock-71          [-1, 512, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]          65,664\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "           Conv2d-74          [-1, 128, 28, 28]         147,584\n",
            "      BatchNorm2d-75          [-1, 128, 28, 28]             256\n",
            "           Conv2d-76          [-1, 512, 28, 28]          66,048\n",
            "      BatchNorm2d-77          [-1, 512, 28, 28]           1,024\n",
            "ResBottleneckBlock-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 128, 28, 28]          65,664\n",
            "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
            "           Conv2d-81          [-1, 128, 28, 28]         147,584\n",
            "      BatchNorm2d-82          [-1, 128, 28, 28]             256\n",
            "           Conv2d-83          [-1, 512, 28, 28]          66,048\n",
            "      BatchNorm2d-84          [-1, 512, 28, 28]           1,024\n",
            "ResBottleneckBlock-85          [-1, 512, 28, 28]               0\n",
            "           Conv2d-86         [-1, 1024, 14, 14]         525,312\n",
            "      BatchNorm2d-87         [-1, 1024, 14, 14]           2,048\n",
            "           Conv2d-88          [-1, 256, 28, 28]         131,328\n",
            "      BatchNorm2d-89          [-1, 256, 28, 28]             512\n",
            "           Conv2d-90          [-1, 256, 14, 14]         590,080\n",
            "      BatchNorm2d-91          [-1, 256, 14, 14]             512\n",
            "           Conv2d-92         [-1, 1024, 14, 14]         263,168\n",
            "      BatchNorm2d-93         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-94         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-95          [-1, 256, 14, 14]         262,400\n",
            "      BatchNorm2d-96          [-1, 256, 14, 14]             512\n",
            "           Conv2d-97          [-1, 256, 14, 14]         590,080\n",
            "      BatchNorm2d-98          [-1, 256, 14, 14]             512\n",
            "           Conv2d-99         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-100         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-101         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-102          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-103          [-1, 256, 14, 14]             512\n",
            "          Conv2d-104          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
            "          Conv2d-106         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-107         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-108         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-109          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-110          [-1, 256, 14, 14]             512\n",
            "          Conv2d-111          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
            "          Conv2d-113         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-114         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-115         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-116          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-117          [-1, 256, 14, 14]             512\n",
            "          Conv2d-118          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-119          [-1, 256, 14, 14]             512\n",
            "          Conv2d-120         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-121         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-122         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-123          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-124          [-1, 256, 14, 14]             512\n",
            "          Conv2d-125          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-129         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-130          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-131          [-1, 256, 14, 14]             512\n",
            "          Conv2d-132          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-133          [-1, 256, 14, 14]             512\n",
            "          Conv2d-134         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-135         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-136         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-137          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-138          [-1, 256, 14, 14]             512\n",
            "          Conv2d-139          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-140          [-1, 256, 14, 14]             512\n",
            "          Conv2d-141         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-142         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-143         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-144          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
            "          Conv2d-146          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-147          [-1, 256, 14, 14]             512\n",
            "          Conv2d-148         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-149         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-150         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-151          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
            "          Conv2d-153          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-154          [-1, 256, 14, 14]             512\n",
            "          Conv2d-155         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-156         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-157         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-158          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-159          [-1, 256, 14, 14]             512\n",
            "          Conv2d-160          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-161          [-1, 256, 14, 14]             512\n",
            "          Conv2d-162         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-163         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-164         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-165          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-166          [-1, 256, 14, 14]             512\n",
            "          Conv2d-167          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-168          [-1, 256, 14, 14]             512\n",
            "          Conv2d-169         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-170         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-171         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-172          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-173          [-1, 256, 14, 14]             512\n",
            "          Conv2d-174          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
            "          Conv2d-176         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-177         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-178         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-179          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-180          [-1, 256, 14, 14]             512\n",
            "          Conv2d-181          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "          Conv2d-183         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-184         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-185         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-186          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-187          [-1, 256, 14, 14]             512\n",
            "          Conv2d-188          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-189          [-1, 256, 14, 14]             512\n",
            "          Conv2d-190         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-191         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-192         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-193          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-194          [-1, 256, 14, 14]             512\n",
            "          Conv2d-195          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-196          [-1, 256, 14, 14]             512\n",
            "          Conv2d-197         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-199         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-200          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-201          [-1, 256, 14, 14]             512\n",
            "          Conv2d-202          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-203          [-1, 256, 14, 14]             512\n",
            "          Conv2d-204         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-205         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-206         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-207          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-208          [-1, 256, 14, 14]             512\n",
            "          Conv2d-209          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-210          [-1, 256, 14, 14]             512\n",
            "          Conv2d-211         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-212         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-213         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-214          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
            "          Conv2d-216          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-217          [-1, 256, 14, 14]             512\n",
            "          Conv2d-218         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-219         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-220         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-221          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
            "          Conv2d-223          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-224          [-1, 256, 14, 14]             512\n",
            "          Conv2d-225         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-226         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-227         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-228          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-229          [-1, 256, 14, 14]             512\n",
            "          Conv2d-230          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-231          [-1, 256, 14, 14]             512\n",
            "          Conv2d-232         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-233         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-234         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-235          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-236          [-1, 256, 14, 14]             512\n",
            "          Conv2d-237          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-238          [-1, 256, 14, 14]             512\n",
            "          Conv2d-239         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-240         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-241         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-242          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-243          [-1, 256, 14, 14]             512\n",
            "          Conv2d-244          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
            "          Conv2d-246         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-247         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-248         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-249          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-250          [-1, 256, 14, 14]             512\n",
            "          Conv2d-251          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
            "          Conv2d-253         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-254         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-255         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-256          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-257          [-1, 256, 14, 14]             512\n",
            "          Conv2d-258          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-259          [-1, 256, 14, 14]             512\n",
            "          Conv2d-260         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-261         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-262         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-263          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-264          [-1, 256, 14, 14]             512\n",
            "          Conv2d-265          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-266          [-1, 256, 14, 14]             512\n",
            "          Conv2d-267         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-269         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-270          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-271          [-1, 256, 14, 14]             512\n",
            "          Conv2d-272          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-273          [-1, 256, 14, 14]             512\n",
            "          Conv2d-274         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-275         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-276         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-277          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-278          [-1, 256, 14, 14]             512\n",
            "          Conv2d-279          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-280          [-1, 256, 14, 14]             512\n",
            "          Conv2d-281         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-282         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-283         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-284          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
            "          Conv2d-286          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-287          [-1, 256, 14, 14]             512\n",
            "          Conv2d-288         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-289         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-290         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-291          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
            "          Conv2d-293          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-294          [-1, 256, 14, 14]             512\n",
            "          Conv2d-295         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-296         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-297         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-298          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-299          [-1, 256, 14, 14]             512\n",
            "          Conv2d-300          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-301          [-1, 256, 14, 14]             512\n",
            "          Conv2d-302         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-303         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-304         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-305          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-306          [-1, 256, 14, 14]             512\n",
            "          Conv2d-307          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-308          [-1, 256, 14, 14]             512\n",
            "          Conv2d-309         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-310         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-311         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-312          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-313          [-1, 256, 14, 14]             512\n",
            "          Conv2d-314          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-315          [-1, 256, 14, 14]             512\n",
            "          Conv2d-316         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-317         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-318         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-319          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-320          [-1, 256, 14, 14]             512\n",
            "          Conv2d-321          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-322          [-1, 256, 14, 14]             512\n",
            "          Conv2d-323         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-324         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-325         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-326          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-327          [-1, 256, 14, 14]             512\n",
            "          Conv2d-328          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-329          [-1, 256, 14, 14]             512\n",
            "          Conv2d-330         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-331         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-332         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-333          [-1, 256, 14, 14]         262,400\n",
            "     BatchNorm2d-334          [-1, 256, 14, 14]             512\n",
            "          Conv2d-335          [-1, 256, 14, 14]         590,080\n",
            "     BatchNorm2d-336          [-1, 256, 14, 14]             512\n",
            "          Conv2d-337         [-1, 1024, 14, 14]         263,168\n",
            "     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n",
            "ResBottleneckBlock-339         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-340           [-1, 2048, 7, 7]       2,099,200\n",
            "     BatchNorm2d-341           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-342          [-1, 512, 14, 14]         524,800\n",
            "     BatchNorm2d-343          [-1, 512, 14, 14]           1,024\n",
            "          Conv2d-344            [-1, 512, 7, 7]       2,359,808\n",
            "     BatchNorm2d-345            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-346           [-1, 2048, 7, 7]       1,050,624\n",
            "     BatchNorm2d-347           [-1, 2048, 7, 7]           4,096\n",
            "ResBottleneckBlock-348           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-349            [-1, 512, 7, 7]       1,049,088\n",
            "     BatchNorm2d-350            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-351            [-1, 512, 7, 7]       2,359,808\n",
            "     BatchNorm2d-352            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-353           [-1, 2048, 7, 7]       1,050,624\n",
            "     BatchNorm2d-354           [-1, 2048, 7, 7]           4,096\n",
            "ResBottleneckBlock-355           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-356            [-1, 512, 7, 7]       1,049,088\n",
            "     BatchNorm2d-357            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-358            [-1, 512, 7, 7]       2,359,808\n",
            "     BatchNorm2d-359            [-1, 512, 7, 7]           1,024\n",
            "          Conv2d-360           [-1, 2048, 7, 7]       1,050,624\n",
            "     BatchNorm2d-361           [-1, 2048, 7, 7]           4,096\n",
            "ResBottleneckBlock-362           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-363           [-1, 2048, 1, 1]               0\n",
            "          Linear-364                 [-1, 1000]       2,049,000\n",
            "================================================================\n",
            "Total params: 61,061,928\n",
            "Trainable params: 61,061,928\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 49.00\n",
            "Forward/backward pass size (MB): 442.94\n",
            "Params size (MB): 232.93\n",
            "Estimated Total Size (MB): 724.87\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet152.parameters(), lr= 0.1, momentum = 0.9)\n",
        "num_epochs = 2\n",
        "resnet152 = resnet152.double()"
      ],
      "metadata": {
        "id": "6uwZjw26TBEj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(my_dataloader):\n",
        "        \n",
        "        train = Variable(images).to(device)\n",
        "        labels = Variable(labels).to(device)\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        train = train.squeeze()\n",
        "        outputs = resnet152(train)\n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = criterion(outputs, labels.long())\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        print(count)\n",
        "        if count % 50 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in my_dataloader:\n",
        "                \n",
        "                test = Variable(images).to(device)\n",
        "                test = test.squeeze()\n",
        "                labels = Variable(labels).to(device)\n",
        "\n",
        "                # Forward propagation\n",
        "                outputs = resnet152(test)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += len(labels)\n",
        "                correct += (predicted == labels).sum()\n",
        "                print(count)\n",
        "            accuracy = 100 * correct / float(total)\n",
        "            \n",
        "            # store loss and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        if count % 100 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk-4N_YFUFE6",
        "outputId": "c19a5d6e-0d28-427c-bd37-e34e3a2838b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "Iteration: 100  Loss: 1.3025030649814127  Accuracy: 42.2092170715332 %\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n",
            "150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ],
      "metadata": {
        "id": "ZdCAIRmRIgr3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}